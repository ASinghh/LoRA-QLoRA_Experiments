{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2d38f3f8-0dd9-4f09-b87a-6b6c19f083c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, load_from_disk\n",
    "from peft import LoraConfig, get_peft_model\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    AutoTokenizer,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    ")\n",
    "import requests\n",
    "import os\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6aa85ddc-9419-46f3-89a3-941af335891b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Status: 200\n",
      "Size: 30288272\n"
     ]
    }
   ],
   "source": [
    "url = \"https://rajpurkar.github.io/SQuAD-explorer/dataset/train-v1.1.json\"\n",
    "r = requests.get(url)\n",
    "print(\"Status:\", r.status_code)\n",
    "with open(\"simple_squad.json\", \"wb\") as f:\n",
    "    f.write(r.content)\n",
    "print(\"Size:\", os.path.getsize(\"simple_squad.json\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c7d2be4e-bc51-492e-af0d-f0eff405ab20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "87599\n"
     ]
    }
   ],
   "source": [
    "OUTPUT_JSONL_FILENAME = \"simple_squad.jsonl\"\n",
    "DOWNLOAD_FILENAME = \"simple_squad.json\"\n",
    "processed_count = 0\n",
    "with open(DOWNLOAD_FILENAME, 'r', encoding='utf-8') as infile, open(OUTPUT_JSONL_FILENAME, 'w', encoding='utf-8') as outfile:\n",
    "    squad_data = json.load(infile)\n",
    "    for topic in squad_data['data']:\n",
    "        for paragraph in topic['paragraphs']:\n",
    "            for qa in paragraph['qas']:\n",
    "                question = qa['question'].strip()\n",
    "                # Ensure there are answers and grab the first one\n",
    "                if qa['answers']:\n",
    "                    answer = qa['answers'][0]['text'].strip()\n",
    "\n",
    "                    # Create the dictionary for the JSONL entry\n",
    "                    entry = {\"question\": question, \"answer\": answer}\n",
    "\n",
    "                    # Write the entry as a JSON string followed by a newline\n",
    "                    outfile.write(json.dumps(entry) + '\\n')\n",
    "                    processed_count += 1\n",
    "print(processed_count)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "740b1bcd-6272-419b-bb77-f5697c51d365",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = 128\n",
    "\n",
    "\n",
    "# Model loading params\n",
    "load_in_4bit = False\n",
    "\n",
    "# LoRA Params\n",
    "lora_alpha = 16             # How much to weigh LoRA params over pretrained params\n",
    "lora_dropout = 0.1          # Dropout for LoRA weights to avoid overfitting\n",
    "lora_r = 16                 # Bottleneck size between A and B matrix for LoRA params\n",
    "lora_bias = \"all\"           # \"all\" or \"none\" for LoRA bias\n",
    "model_type = \"wizard7\"        # falcon or llama or wizard7 or wizard13\n",
    "dataset_type = \"squad\"      # \"squad\" or \"reddit\" or \"reddit_negative\"\n",
    "lora_target_modules = [     # Which modules to apply LoRA to (names of the modules in state_dict)\n",
    "    \"query_key_value\",\n",
    "    \"dense\",\n",
    "    \"dense_h_to_4h\",\n",
    "    \"dense_4h_to_h\",\n",
    "] if model_type == \"falcon\" else [\n",
    "    \"q_proj\",\n",
    "    \"k_proj\",\n",
    "    \"v_proj\",\n",
    "    \"o_proj\",\n",
    "    \"gate_proj\",\n",
    "    \"up_proj\",\n",
    "    \"down_proj\"\n",
    "]\n",
    "\n",
    "# Trainer params\n",
    "output_dir = \"outputs_squad\"                              # Directory to save the model\n",
    "optim_type = \"adafactor\"                            # Optimizer type to train with \n",
    "learning_rate = 0.00005                              # Model learning rate\n",
    "weight_decay = 0.002                                # Model weight decay\n",
    "per_device_train_batch_size = 8                     # Train batch size on each GPU\n",
    "per_device_eval_batch_size = 8                      # Eval batch size on each GPU\n",
    "gradient_accumulation_steps = 2                     # Number of steps before updating model\n",
    "warmup_steps = 5                                    # Number of warmup steps for learning rate\n",
    "save_steps = 100                                     # Number of steps before saving model\n",
    "logging_steps = 25                                  # Number of steps before logging\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9c00909e-ee1e-4d67-bf6c-4da8972e249a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama_fast.LlamaTokenizerFast'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "          \"TheBloke/wizardLM-7B-HF\",\n",
    "          trust_remote_code=True,\n",
    "         cache_dir=\"./models\",\n",
    "          )\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1f039323-6339-4bdf-acc6-b94d90e5e74e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_function(example):\n",
    "        # Get the question and model output\n",
    "        question = f\"#### Human: {example['question'].strip()}\"\n",
    "        output = f\"#### Assistant: {example['answer'].strip()}\"\n",
    "\n",
    "        # Encode the question and output\n",
    "        question_encoded = tokenizer(question)\n",
    "        output_encoded = tokenizer(output, max_length=max_length-1-len(question_encoded[\"input_ids\"]), truncation=True, padding=\"max_length\")\n",
    "\n",
    "        # Add on a pad token to the end of the input_ids\n",
    "        output_encoded[\"input_ids\"] = output_encoded[\"input_ids\"] + [tokenizer.pad_token_id]\n",
    "        output_encoded[\"attention_mask\"] = output_encoded[\"attention_mask\"] + [0]\n",
    "\n",
    "        # Combine the input ids\n",
    "        input_ids = question_encoded[\"input_ids\"] + output_encoded[\"input_ids\"]\n",
    "\n",
    "        # # The labels are the input ids, but we want to mask the loss for the context and padding\n",
    "        # labels = [-100]*len(question_encoded[\"input_ids\"]) + [output_encoded[\"input_ids\"][i] if output_encoded[\"attention_mask\"][i] == 1 else -100 for i in range(len(output_encoded[\"attention_mask\"]))]\n",
    "\n",
    "        # Combine the attention masks. Attention masks are 0\n",
    "        # where we want to mask and 1 where we want to attend.\n",
    "        # We want to attend to both context and generated output\n",
    "        # Also add a 1 for a single padding\n",
    "        attention_mask = [1]*len(question_encoded[\"input_ids\"]) + [1]*(sum(output_encoded[\"attention_mask\"])+1) + [0]*(len(output_encoded[\"attention_mask\"])-sum(output_encoded[\"attention_mask\"])-1)\n",
    "        \n",
    "        # The labels are the input ids, but we want to mask the loss for the context and padding\n",
    "        labels = [input_ids[i] if attention_mask[i] == 1 else -100 for i in range(len(attention_mask))]\n",
    "        assert len(labels) == len(attention_mask) and len(attention_mask) == len(input_ids), \"Labels is not the correct length\"\n",
    "\n",
    "        return {\n",
    "            \"input_ids\": input_ids,\n",
    "            \"labels\": labels,\n",
    "            \"attention_mask\": attention_mask\n",
    "             }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0495c89b-00d7-41d4-ba61-49f4e81b1752",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, DatasetDict\n",
    "\n",
    "jsonl_file_path = \"simple_squad.jsonl\"  \n",
    "test_split_percentage = 0.2 \n",
    "random_seed = 42\n",
    "dataset = load_dataset(\"json\", data_files=jsonl_file_path)\n",
    "dataset = dataset[\"train\"].map(map_function)\n",
    "dataset = dataset.shuffle()\n",
    "train_size = int(len(dataset) * 0.8)\n",
    "test_size = len(dataset) - train_size\n",
    "data_train = dataset.select(range(train_size))\n",
    "data_test = dataset.select(range(train_size, train_size + test_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "be124b48-31bc-4ece-a552-bb82c88e7ae6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c41513d9c26e4febbdf7417fe2575da6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin.index.json:   0%|          | 0.00/26.8k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "39825678d82f467c8263e7b49867b79f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 2 files:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b306b69316a94e058bdda595ad410a5a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model-00001-of-00002.bin:   0%|          | 0.00/9.98G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ebe3334f8418429e90b905f90ac082ff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model-00002-of-00002.bin:   0%|          | 0.00/3.50G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a8dae91621b342bf9964967510cc4284",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/28.1k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "268b79a240c743e0a2f5a68a6051fb3c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3dc207ca0d0e4b48b126cbfddba6a069",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/132 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "bnb_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_quant_type=\"nf4\",\n",
    "        bnb_4bit_compute_dtype=\"float16\",\n",
    "        bnb_4bit_use_double_quant=True,\n",
    "    )\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "         \"TheBloke/wizardLM-7B-HF\", \n",
    "        trust_remote_code=True, \n",
    "        device_map=\"auto\", \n",
    "        quantization_config=bnb_config,\n",
    "        cache_dir=\"./models\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0b3c52e6-9b3f-4c91-b0ef-2fd2701ccb56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 39,976,960 || all params: 6,778,400,768 || trainable%: 0.5898\n"
     ]
    }
   ],
   "source": [
    "peft_config = LoraConfig(\n",
    "    lora_alpha=lora_alpha,\n",
    "    lora_dropout=lora_dropout,\n",
    "    r=lora_r,\n",
    "    bias=lora_bias,\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    inference_mode=False,\n",
    "    target_modules=lora_target_modules\n",
    ")\n",
    "model = get_peft_model(model, peft_config)\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2903960d-b8d2-4ab3-acf9-456d842c0e10",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_871/555917835.py:15: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1684' max='13140' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 1684/13140 48:07 < 5:27:44, 0.58 it/s, Epoch 0.38/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "36254d4faae24df0aaa2095cc98746b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/555 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    eval_strategy=\"epoch\",\n",
    "    optim=optim_type,\n",
    "    learning_rate=learning_rate,\n",
    "    weight_decay=weight_decay,\n",
    "    per_device_train_batch_size=per_device_train_batch_size,\n",
    "    per_device_eval_batch_size=per_device_eval_batch_size,\n",
    "    gradient_accumulation_steps=gradient_accumulation_steps,\n",
    "    do_train=True,\n",
    "    warmup_steps=warmup_steps,\n",
    "    save_steps=save_steps,\n",
    "    logging_steps=logging_steps,\n",
    ")\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=data_train,\n",
    "    eval_dataset=data_test,\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8622daa1-82d4-43e8-9168-8d7e67ec25d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import peft\n",
    "import torch\n",
    "from peft import PeftConfig, PeftModel\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, HfArgumentParser\n",
    "import shutil\n",
    "\n",
    "\n",
    "\n",
    "lora_path = \"outputs_squad/checkpoint-12200\" # Path to the LoRA weights\n",
    "output_path = \"outputs_squad/merged_model\"   # Path to output the merged weights\n",
    "model_type = \"wizard7\"    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b60ac1e0-4410-489d-991a-59235eb5cc97",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c1c8ed962b64b118be0f3de346fb2e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "peft_model_id = lora_path\n",
    "peft_config = PeftConfig.from_pretrained(peft_model_id)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    peft_config.base_model_name_or_path,\n",
    "    return_dict=True,\n",
    "    torch_dtype=torch.float16,\n",
    "    trust_remote_code=True,\n",
    "    device_map=\"cpu\",\n",
    "    cache_dir=\"./models\"\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(peft_config.base_model_name_or_path, cache_dir=\"./models\")\n",
    "\n",
    "\n",
    "import os\n",
    "\n",
    "path = os.listdir(\"models/models--TheBloke--wizardLM-7B-HF/snapshots\")[0]\n",
    "path = os.path.join(\"models/models--TheBloke--wizardLM-7B-HF/snapshots\", path)\n",
    "model = PeftModel.from_pretrained(model, peft_model_id)\n",
    "model.eval()\n",
    "key_list = [key for key, _ in model.named_modules() if \"lora\" not in key]\n",
    "for key in key_list:\n",
    "    try:\n",
    "        sub_mod = model.get_submodule(key)\n",
    "        parent = model.get_submodule(\".\".join(key.split(\".\")[:-1]))\n",
    "    except AttributeError:\n",
    "        continue\n",
    "    target_name = key.split(\".\")[-1]\n",
    "    if isinstance(sub_mod, peft.tuners.lora.Linear):\n",
    "        sub_mod.merge()\n",
    "        bias = sub_mod.bias is not None\n",
    "        new_module = torch.nn.Linear(sub_mod.in_features, sub_mod.out_features, bias=bias)\n",
    "        new_module.weight.data = sub_mod.weight\n",
    "        if bias:\n",
    "            new_module.bias.data = sub_mod.bias\n",
    "        model.base_model._replace_module(parent, target_name, new_module, sub_mod)\n",
    "\n",
    "model = model.base_model.model\n",
    "\n",
    "# Save the model\n",
    "model.save_pretrained(output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "16a41f09-8fb9-4fc7-b9b0-1595168e2bb6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d4747c3b48494718a680a64e32bf3412",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "OSError",
     "evalue": "Can't load tokenizer for 'outputs_squad/merged_model'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'outputs_squad/merged_model' is the correct path to a directory containing all relevant files for a LlamaTokenizerFast tokenizer.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 26\u001b[0m\n\u001b[1;32m     12\u001b[0m bnb_config \u001b[38;5;241m=\u001b[39m BitsAndBytesConfig(\n\u001b[1;32m     13\u001b[0m         load_in_4bit\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m     14\u001b[0m         bnb_4bit_quant_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnf4\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     15\u001b[0m         bnb_4bit_compute_dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfloat16\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     16\u001b[0m         bnb_4bit_use_double_quant\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m     17\u001b[0m     )\n\u001b[1;32m     18\u001b[0m model \u001b[38;5;241m=\u001b[39m AutoModelForCausalLM\u001b[38;5;241m.\u001b[39mfrom_pretrained(\n\u001b[1;32m     19\u001b[0m     model_path, \n\u001b[1;32m     20\u001b[0m     trust_remote_code\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     24\u001b[0m     quantization_config\u001b[38;5;241m=\u001b[39mbnb_config \u001b[38;5;28;01mif\u001b[39;00m device \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mauto\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m     25\u001b[0m )\u001b[38;5;241m.\u001b[39meval()\n\u001b[0;32m---> 26\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m \u001b[43mAutoTokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_path\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/auto/tokenization_auto.py:1028\u001b[0m, in \u001b[0;36mAutoTokenizer.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m   1025\u001b[0m tokenizer_class_py, tokenizer_class_fast \u001b[38;5;241m=\u001b[39m TOKENIZER_MAPPING[\u001b[38;5;28mtype\u001b[39m(config)]\n\u001b[1;32m   1027\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m tokenizer_class_fast \u001b[38;5;129;01mand\u001b[39;00m (use_fast \u001b[38;5;129;01mor\u001b[39;00m tokenizer_class_py \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m-> 1028\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtokenizer_class_fast\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1029\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1030\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m tokenizer_class_py \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/tokenization_utils_base.py:2046\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, cache_dir, force_download, local_files_only, token, revision, trust_remote_code, *init_inputs, **kwargs)\u001b[0m\n\u001b[1;32m   2043\u001b[0m \u001b[38;5;66;03m# If one passes a GGUF file path to `gguf_file` there is no need for this check as the tokenizer will be\u001b[39;00m\n\u001b[1;32m   2044\u001b[0m \u001b[38;5;66;03m# loaded directly from the GGUF file.\u001b[39;00m\n\u001b[1;32m   2045\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mall\u001b[39m(full_file_name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m full_file_name \u001b[38;5;129;01min\u001b[39;00m resolved_vocab_files\u001b[38;5;241m.\u001b[39mvalues()) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m gguf_file:\n\u001b[0;32m-> 2046\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mEnvironmentError\u001b[39;00m(\n\u001b[1;32m   2047\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCan\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt load tokenizer for \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpretrained_model_name_or_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m. If you were trying to load it from \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2048\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttps://huggingface.co/models\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, make sure you don\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt have a local directory with the same name. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2049\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOtherwise, make sure \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpretrained_model_name_or_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m is the correct path to a directory \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2050\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontaining all relevant files for a \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m tokenizer.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2051\u001b[0m     )\n\u001b[1;32m   2053\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m file_id, file_path \u001b[38;5;129;01min\u001b[39;00m vocab_files\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m   2054\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m file_id \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m resolved_vocab_files:\n",
      "\u001b[0;31mOSError\u001b[0m: Can't load tokenizer for 'outputs_squad/merged_model'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'outputs_squad/merged_model' is the correct path to a directory containing all relevant files for a LlamaTokenizerFast tokenizer."
     ]
    }
   ],
   "source": [
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    AutoTokenizer,\n",
    ")\n",
    "import torch\n",
    "\n",
    "device = \"auto\"\n",
    "model_path = \"outputs_squad/merged_model\"            \n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_quant_type=\"nf4\",\n",
    "        bnb_4bit_compute_dtype=\"float16\",\n",
    "        bnb_4bit_use_double_quant=True,\n",
    "    )\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_path, \n",
    "    trust_remote_code=True, \n",
    "    device_map=device, \n",
    "    torch_dtype=torch.bfloat16,\n",
    "    # load_in_8bit=True,\n",
    "    quantization_config=bnb_config if device == \"auto\" else None,\n",
    ").eval()\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c340ec50-3614-42e4-a041-a6f9024fd0c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"#### Human: Who is the president of France?#### Assistant: \"\n",
    "\n",
    "limit = 128\n",
    "\n",
    "\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "if device != \"cpu\":\n",
    "    inputs = inputs.to('cuda')\n",
    "# del inputs['token_type_ids']\n",
    "output = model.generate(**inputs, temperature=0.1, do_sample=True, top_p=0.95, top_k=60, max_new_tokens=limit-len(inputs[\"input_ids\"]), pad_token_id=tokenizer.pad_token_id)\n",
    "output = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "\n",
    "# print(output)\n",
    "print(output.split(\"#### Assistant:\")[1].strip())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
